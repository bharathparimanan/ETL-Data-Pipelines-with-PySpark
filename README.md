# ETL-Data-Pipelines-with-PySpark

A production-inspired data engineering project that demonstrates loading, transforming, and analyzing educational attendance data using PySpark in a modular, reproducible way.

## 📁 Notebooks
- `01_data_exploration_and_summary.ipynb`: Load raw data, perform exploration, and summarize attendance by school and year group.
- `02_etl_pipeline_and_integrity_check.ipynb`: Implements modular ETL logic, reusable transformation methods, joins, and data validation checks.

## 🧱 Tech Stack
- PySpark
- Jupyter
- Pandas (for visualization)
- Parquet & CSV (data formats)

## 📊 Features
- Multi-source data ingestion (CSV, Parquet)
- Join and transformation logic
- Attendance percentage by school/year group
- Data quality and integrity checks

## 📂 Folder Structure
See the `notebooks/`, `docs/`, and `data/` directories for details.

## 🧪 How to Run
1. Install requirements
